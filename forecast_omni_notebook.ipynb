{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space Weather Forecast Notebook\n",
    "\n",
    "This notebook is used for debugging and experimenting with the PatchTST model for forecasting space weather indices (ap_index_nT and f10.7_index).\n",
    "\n",
    "We will load and preprocess the data, define our model and dataset, and then run training and evaluation functions. Use the controls below to run or skip certain sections (e.g., if you only want to debug data preprocessing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Make sure you have tqdm installed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Set up logging with a default INFO level.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# For notebook usage, you might want to display plots inline.\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the PatchTST-inspired Model\n",
    "\n",
    "The model takes a time series input, \"patchifies\" it using a 1D convolution, passes it through Transformer encoder layers, and finally outputs a forecast for the target features. We set `batch_first=True` to simplify the input shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTST(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified PatchTST model.\n",
    "    This model \"patchifies\" the input time series with a 1D convolution,\n",
    "    processes the patches with Transformer encoder layers, and then outputs\n",
    "    a forecast for a fixed horizon.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, patch_size, d_model, n_heads, n_layers, \n",
    "                 forecast_horizon, target_size, dropout=0.1):\n",
    "        super(PatchTST, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.patch_size = patch_size\n",
    "        self.d_model = d_model\n",
    "        self.forecast_horizon = forecast_horizon  # For reshaping output\n",
    "        self.target_size = target_size            # For reshaping output\n",
    "\n",
    "        logger.debug(\"Initializing PatchTST model.\")\n",
    "        # Patchify the time series along the time axis.\n",
    "        self.proj = nn.Conv1d(in_channels=input_size, out_channels=d_model,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "        # Use batch_first=True for the Transformer layers.\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_heads, d_model * 4, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        # Forecast head outputs forecast_horizon steps for each target feature.\n",
    "        self.fc = nn.Linear(d_model, forecast_horizon * target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, input_size)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, forecast_horizon, target_size)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        # Conv1d expects (batch, input_size, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.proj(x)       # (batch, d_model, new_seq_len)\n",
    "        x = x.transpose(1, 2)  # (batch, new_seq_len, d_model)\n",
    "        x = self.transformer_encoder(x)  # (batch, new_seq_len, d_model)\n",
    "        # Use the last patch's representation\n",
    "        x_last = x[:, -1, :]\n",
    "        out = self.fc(x_last)\n",
    "        out = out.view(batch_size, self.forecast_horizon, self.target_size)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Dataset and DataHandler\n",
    "\n",
    "The `TimeSeriesDataset` class creates sliding window examples.  \n",
    "The `DataHandler` class loads OMNI and sat density files based on an initial state file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Creates sliding-window examples from the full time series.\n",
    "    Each sample is a tuple (x, y) where:\n",
    "      - x: a window of input features (length = input_window)\n",
    "      - y: the next forecast_horizon values for the target features.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, Y, input_window, forecast_horizon):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.input_window = input_window\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.length = len(X) - input_window - forecast_horizon + 1\n",
    "        logger.debug(f\"TimeSeriesDataset initialized with {self.length} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx : idx + self.input_window]\n",
    "        y = self.Y[idx + self.input_window : idx + self.input_window + self.forecast_horizon]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "class DataHandler():\n",
    "    def __init__(self, omni_folder, initial_state_folder, sat_density_folder, forcasted_omni2_data_folder):\n",
    "        self.omni_folder = omni_folder\n",
    "        self.initial_state_folder = initial_state_folder\n",
    "        self.sat_density_folder = sat_density_folder\n",
    "        self.forcasted_omni2_data_folder = forcasted_omni2_data_folder\n",
    "        self.__read_initial_states()\n",
    "\n",
    "    def __read_initial_states(self):\n",
    "        all_dataframes = []\n",
    "        for file in self.initial_state_folder.iterdir():\n",
    "            if file.suffix == '.csv':\n",
    "                df = pd.read_csv(file)\n",
    "                all_dataframes.append(df)\n",
    "        self.initial_states = pd.concat(all_dataframes, ignore_index=True)\n",
    "        logger.info(f\"Loaded initial state data with {len(self.initial_states)} rows.\")\n",
    "\n",
    "    def read_omni_data(self, file_id):\n",
    "        file_id_str = f\"{file_id:05d}\"\n",
    "        for file in self.omni_folder.iterdir():\n",
    "            if file.suffix == '.csv' and file_id_str in file.stem:\n",
    "                logger.debug(f\"Reading OMNI file for File ID {file_id}.\")\n",
    "                return pd.read_csv(file)\n",
    "        raise FileNotFoundError(f\"File with ID {file_id} not found in {self.omni_folder}\")\n",
    "\n",
    "    def read_sat_density_data(self, file_id):\n",
    "        file_id_str = f\"{file_id:05d}\"\n",
    "        for file in self.sat_density_folder.iterdir():\n",
    "            if file.suffix == '.csv' and file_id_str in file.stem:\n",
    "                logger.debug(f\"Reading Sat density file for File ID {file_id}.\")\n",
    "                return pd.read_csv(file)\n",
    "        raise FileNotFoundError(f\"File with ID {file_id} not found in {self.sat_density_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training, Evaluation, and Forecasting Functions\n",
    "\n",
    "This cell contains the training loop (with progress reporting via `tqdm`), evaluation functions, and a helper function for forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, lr, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.to(device)\n",
    "    logger.info(\"Starting training loop.\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_pbar = tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "                          desc=f\"Epoch {epoch+1}/{epochs} Training\", unit=\"batch\")\n",
    "        for batch_idx, (x, y) in train_pbar:\n",
    "            # Check for NaNs in inputs\n",
    "            if torch.isnan(x).any():\n",
    "                logger.error(f\"Input batch contains NaNs. Batch {batch_idx} at epoch {epoch+1}\")\n",
    "            if torch.isnan(y).any():\n",
    "                logger.error(f\"Target batch contains NaNs. Batch {batch_idx} at epoch {epoch+1}\")\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            if torch.isnan(loss):\n",
    "                logger.error(f\"Loss is NaN at epoch {epoch+1}, batch {batch_idx}. Aborting training.\")\n",
    "                return\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_pbar = tqdm(enumerate(val_loader), total=len(val_loader),\n",
    "                        desc=f\"Epoch {epoch+1}/{epochs} Validation\", unit=\"batch\")\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x, y) in val_pbar:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item() * x.size(0)\n",
    "                val_pbar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        logger.info(f\"Epoch {epoch+1}/{epochs}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    mse_sum = 0.0\n",
    "    mae_sum = 0.0\n",
    "    total_elements = 0\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    logger.info(\"Starting evaluation.\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(test_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_trues.append(y.cpu())\n",
    "            mse_sum += criterion(preds, y).item()\n",
    "            mae_sum += torch.sum(torch.abs(preds - y)).item()\n",
    "            total_elements += y.numel()\n",
    "            logger.debug(f\"Batch {batch_idx+1}/{len(test_loader)}: MSE={mse_sum/total_elements:.6f}, MAE={mae_avg/total_elements:.6f}\")\n",
    "    \n",
    "    mse_avg = mse_sum / total_elements\n",
    "    mae_avg = mae_sum / total_elements\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_trues = torch.cat(all_trues, dim=0)\n",
    "    logger.info(f\"Evaluation complete: MSE={mse_avg:.6f}, MAE={mae_avg:.6f}\")\n",
    "    \n",
    "    return mse_avg, mae_avg, all_preds, all_trues\n",
    "\n",
    "def forecast(model, input_sequence, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(input_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        output = model(input_tensor)\n",
    "    return output.squeeze(0).cpu().numpy()\n",
    "\n",
    "def plot_combined_forecast(history_timestamps, history_values, forecast_timestamps, forecast_values, \n",
    "                           ylabel, title, true_forecast=None):\n",
    "    \"\"\"\n",
    "    Plot a combined time series showing historical data and forecasted values.\n",
    "    Optionally, also plot true forecast values in the background.\n",
    "    \n",
    "    Args:\n",
    "        history_timestamps (array-like): Timestamps for historical data.\n",
    "        history_values (array-like): Historical target values.\n",
    "        forecast_timestamps (array-like): Timestamps for forecasted data.\n",
    "        forecast_values (array-like): Forecasted target values.\n",
    "        ylabel (str): Label for the Y-axis.\n",
    "        title (str): Plot title.\n",
    "        true_forecast (array-like, optional): True forecast values.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Plotting combined forecast.\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot true forecast first (background)\n",
    "    if true_forecast is not None:\n",
    "        plt.plot(forecast_timestamps, true_forecast, \n",
    "                 label=\"True Forecast\", marker='o', linestyle='--', color='green', \n",
    "                 zorder=1, alpha=0.5)\n",
    "    \n",
    "    # Plot historical and predicted forecast on top (foreground)\n",
    "    plt.plot(history_timestamps, history_values, label=\"Historical\", marker='o', color='blue', zorder=2)\n",
    "    plt.plot(forecast_timestamps, forecast_values, label=\"Forecast\", marker='x', color='red', zorder=2)\n",
    "    \n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Use a DateFormatter to show full timestamp with hour, minute, and second.\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading, Preprocessing, and Feature Selection\n",
    "\n",
    "In this section, we:\n",
    "- Set folder paths and load the data using `DataHandler`.\n",
    "- Load a subset (using `load_percentage`) for faster debugging.\n",
    "- Preprocess the data: convert timestamps, sort, and select only the suggested candidate features.\n",
    "- Remove any features that are all NaN or constant.\n",
    "- Fill any remaining NaNs with the median and normalize the input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folder paths (adjust as needed)\n",
    "omni_folder = Path(\"./data/omni2\")\n",
    "initial_state_folder = Path(\"./data/initial_state\")\n",
    "sat_density_folder = Path(\"./data/sat_density\")\n",
    "forcasted_omni2_data_folder = Path(\"./data/forcasted_omni2\")\n",
    "\n",
    "# Instantiate DataHandler.\n",
    "data_handler = DataHandler(omni_folder, initial_state_folder, sat_density_folder, forcasted_omni2_data_folder)\n",
    "\n",
    "# Load all files based on initial_states File ID.\n",
    "load_percentage = 0.05  # For testing, load only 1% of file IDs.\n",
    "\n",
    "print(data_handler.initial_states.columns)\n",
    "\n",
    "file_ids = data_handler.initial_states[\"File ID\"].unique()\n",
    "logger.info(f\"Total file IDs available: {len(file_ids)}\")\n",
    "if load_percentage < 1.0:\n",
    "    file_ids = np.random.choice(file_ids, size=int(load_percentage * len(file_ids)), replace=False)\n",
    "    logger.info(f\"Randomly selected {len(file_ids)} file IDs ({load_percentage*100:.0f}%) for loading.\")\n",
    "\n",
    "omni_dfs = []\n",
    "sat_density_dfs = []\n",
    "total_ids = len(file_ids)\n",
    "next_threshold = 0.05  # 5% increments\n",
    "\n",
    "for i, fid in enumerate(file_ids):\n",
    "    try:\n",
    "        omni_dfs.append(data_handler.read_omni_data(fid))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"OMNI file for File ID {fid} not found; skipping.\")\n",
    "    try:\n",
    "        sat_density_dfs.append(data_handler.read_sat_density_data(fid))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Sat density file for File ID {fid} not found; skipping.\")\n",
    "    progress = (i + 1) / total_ids\n",
    "    if progress >= next_threshold:\n",
    "        print(f\"{int(next_threshold * 100)}% complete\")\n",
    "        next_threshold += 0.05\n",
    "\n",
    "if len(omni_dfs) == 0 or len(sat_density_dfs) == 0:\n",
    "    raise ValueError(\"No valid OMNI or sat_density files were loaded.\")\n",
    "omni_df = pd.concat(omni_dfs, ignore_index=True)\n",
    "sat_density_df = pd.concat(sat_density_dfs, ignore_index=True)\n",
    "logger.info(f\"Loaded {len(omni_df)} rows of OMNI data and {len(sat_density_df)} rows of sat density data.\")\n",
    "\n",
    "# Preprocess: convert Timestamps and sort.\n",
    "omni_df['Timestamp'] = pd.to_datetime(omni_df['Timestamp'])\n",
    "omni_df.sort_values('Timestamp', inplace=True)\n",
    "sat_density_df['Timestamp'] = pd.to_datetime(sat_density_df['Timestamp'])\n",
    "sat_density_df.sort_values('Timestamp', inplace=True)\n",
    "logger.debug(\"Timestamps converted and sorted.\")\n",
    "\n",
    "# Define input and target features.\n",
    "ap_features = [\n",
    "    \"Kp_index\", \"Dst_index_nT\", \"AU_index_nT\", \"AL_index_nT\", \"AE_index_nT\",\n",
    "    \"SW_Plasma_Speed_km_s\", \"SW_Proton_Density_N_cm3\", \"SW_Plasma_Temperature_K\",\n",
    "    \"Scalar_B_nT\", \"Vector_B_Magnitude_nT\", \"BX_nT_GSE_GSM\", \"BY_nT_GSE\", \"BZ_nT_GSE\",\n",
    "    \"Plasma_Beta\", \"Flow_pressure\"\n",
    "]\n",
    "f107_features = [\n",
    "    \"Bartels_rotation_number\", \"R_Sunspot_No\", \"Lyman_alpha\"\n",
    "]\n",
    "candidate_features = list(set(ap_features).union(set(f107_features)))\n",
    "candidate_features = [f for f in candidate_features if f in omni_df.columns]\n",
    "\n",
    "# Remove features that are all NaN or constant.\n",
    "cleaned_features = []\n",
    "for f in candidate_features:\n",
    "    if omni_df[f].isna().all():\n",
    "        logger.debug(f\"Feature {f} is all NaN, removing.\")\n",
    "        continue\n",
    "    if omni_df[f].nunique() <= 1:\n",
    "        logger.debug(f\"Feature {f} is constant, removing.\")\n",
    "        continue\n",
    "    cleaned_features.append(f)\n",
    "\n",
    "input_features = cleaned_features\n",
    "target_features = [\"ap_index_nT\", \"f10.7_index\"]\n",
    "\n",
    "logger.info(f\"Selected input features: {input_features}\")\n",
    "logger.info(f\"Target features: {target_features}\")\n",
    "\n",
    "# Fill remaining NaN values in input features with their median.\n",
    "for col in input_features:\n",
    "    if omni_df[col].isna().any():\n",
    "        median_val = omni_df[col].median()\n",
    "        logger.debug(f\"Filling NaNs in {col} with median value {median_val}.\")\n",
    "        omni_df[col] = omni_df[col].fillna(median_val)\n",
    "\n",
    "# Normalize the input features.\n",
    "input_data = omni_df[input_features]\n",
    "input_data = (input_data - input_data.mean()) / input_data.std()\n",
    "omni_df[input_features] = input_data\n",
    "\n",
    "# Create input (X) and target (Y) arrays.\n",
    "X = omni_df[input_features].values\n",
    "Y = omni_df[target_features].values\n",
    "logger.info(f\"Input features shape: {X.shape}, Target features shape: {Y.shape}\")\n",
    "\n",
    "# Forecasting parameters.\n",
    "input_window = 100  # e.g., 100 timesteps\n",
    "unique_timestamps = sat_density_df[\"Timestamp\"].drop_duplicates().reset_index(drop=True)\n",
    "forecast_horizon = len(unique_timestamps)\n",
    "logger.info(f\"Using input_window={input_window} and forecast_horizon={forecast_horizon}\")\n",
    "\n",
    "# Train-Test (80/20) split.\n",
    "split_idx = int(0.8 * len(X))\n",
    "logger.info(f\"Train/Test split at index {split_idx} out of {len(X)} samples.\")\n",
    "train_X = X[:split_idx]\n",
    "train_Y = Y[:split_idx]\n",
    "test_X = X[split_idx - input_window - forecast_horizon + 1:]\n",
    "test_Y = Y[split_idx - input_window - forecast_horizon + 1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "This cell instantiates the model and data loaders, then starts the training loop.\n",
    "Note that training may take some time. If you only want to debug or test without training every time, you can comment out the training call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Instantiate the model.\n",
    "patch_size = 10\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "epochs = 10  # Adjust as needed\n",
    "lr = 1e-3\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_X, train_Y, input_window, forecast_horizon)\n",
    "test_dataset = TimeSeriesDataset(test_X, test_Y, input_window, forecast_horizon)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "logger.info(f\"Train dataset length: {len(train_dataset)}, Test dataset length: {len(test_dataset)}\")\n",
    "\n",
    "model = PatchTST(input_size=len(input_features), patch_size=patch_size, \n",
    "                 d_model=d_model, n_heads=n_heads, n_layers=n_layers, \n",
    "                 forecast_horizon=forecast_horizon, target_size=len(target_features), \n",
    "                 dropout=dropout)\n",
    "logger.debug(\"Model instantiated.\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(\"Cuda available: True\")\n",
    "else:\n",
    "    raise ValueError(\"Cuda not available\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "logger.info(\"Starting training...\")\n",
    "# Uncomment the next line to run training.\n",
    "train_model(model, train_loader, test_loader, epochs, lr, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "This cell evaluates the model on the test set and prints out the metrics (MSE and MAE).\n",
    "Make sure that training has been completed (or load a pre-trained model) before running this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines to run evaluation.\n",
    "test_mse, test_mae, preds, trues = evaluate_model(model, test_loader, device)\n",
    "logger.info(f\"Test MSE: {test_mse:.6f}, Test MAE: {test_mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Forecasting and Plotting\n",
    "\n",
    "In this cell, we select a test sample, compute a forecast, and plot the historical data together with the forecasted data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def plot_60_days_and_3_day_forecast(\n",
    "    history_timestamps, history_ap, history_f107,\n",
    "    forecast_timestamps, forecast_ap, forecast_f107\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots 60 days of historical data for ap_index_nT (red) and f10.7_index (blue),\n",
    "    plus 3 days of forecasted data (dashed lines) at satellite-density timestamps.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # --- Plot Historical Data ---\n",
    "    ax.plot(history_timestamps, history_ap, color='red', label='ap (history)')\n",
    "    ax.plot(history_timestamps, history_f107, color='blue', label='f10.7 (history)')\n",
    "\n",
    "    # --- Plot Forecasted Data ---\n",
    "    ax.plot(forecast_timestamps, forecast_ap, color='magenta', linestyle='--', label='ap (forecast)')\n",
    "    ax.plot(forecast_timestamps, forecast_f107, color='cyan', linestyle='--', label='f10.7 (forecast)')\n",
    "\n",
    "    # Format the x-axis to show full date + time\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"Index Value\")\n",
    "    plt.title(\"60 Days Historical + 3 Days Forecast for ap_index_nT and f10.7_index\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "test_offset = split_idx - input_window - forecast_horizon + 1\n",
    "sample_index = 0  # Choose the first test sample (change as needed)\n",
    "global_index = test_offset + sample_index\n",
    "logger.debug(f\"Using global index {global_index} for forecast sample.\")\n",
    "\n",
    "history_timestamps = omni_df['Timestamp'].iloc[global_index : global_index + input_window].values\n",
    "history_ap = omni_df['ap_index_nT'].iloc[global_index : global_index + input_window].values\n",
    "history_f107 = omni_df['f10.7_index'].iloc[global_index : global_index + input_window].values\n",
    "\n",
    "forecast_timestamps = omni_df['Timestamp'].iloc[global_index + input_window : global_index + input_window + forecast_horizon].values\n",
    "true_forecast_ap = omni_df['ap_index_nT'].iloc[global_index + input_window : global_index + input_window + forecast_horizon].values\n",
    "true_forecast_f107 = omni_df['f10.7_index'].iloc[global_index + input_window : global_index + input_window + forecast_horizon].values\n",
    "\n",
    "# Run your model to get predictions\n",
    "sample_x, _ = test_dataset[sample_index]\n",
    "sample_x = sample_x.unsqueeze(0).to(device)\n",
    "pred_sample = model(sample_x).cpu().detach().numpy().squeeze(0)\n",
    "pred_ap = pred_sample[:, 0]\n",
    "pred_f107 = pred_sample[:, 1]\n",
    "logger.info(\"Forecast sample computed.\")\n",
    "\n",
    "# Plot the forecast for ap_index_nT.\n",
    "plot_combined_forecast(history_timestamps, history_ap, forecast_timestamps, pred_ap,\n",
    "                       ylabel=\"ap_index_nT\", title=\"Historical and Forecasted ap_index_nT\",\n",
    "                       true_forecast=true_forecast_ap)\n",
    "\n",
    "# Plot the forecast for f10.7_index.\n",
    "plot_combined_forecast(history_timestamps, history_f107, forecast_timestamps, pred_f107,\n",
    "                       ylabel=\"f10.7_index\", title=\"Historical and Forecasted f10.7_index\",\n",
    "                       true_forecast=true_forecast_f107)\n",
    "\n",
    "plot_60_days_and_3_day_forecast(\n",
    "    history_timestamps, history_ap, history_f107,\n",
    "    forecast_timestamps, pred_ap, pred_f107\n",
    ")\n",
    "\n",
    "logger.info(\"Plots generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "def plot_forecast_for_file(omni_df, predicted_sat_df, target, history_days=60):\n",
    "    \"\"\"\n",
    "    Plots the historical values for a target from omni_df (for the 60 days preceding the\n",
    "    forecast) and the predicted values from predicted_sat_df (at the sat density timestamps).\n",
    "\n",
    "    Args:\n",
    "        omni_df (DataFrame): The entire OMNI DataFrame (must have a 'Timestamp' column).\n",
    "        predicted_sat_df (DataFrame): The sat density DataFrame for a single file_id,\n",
    "            with columns \"Timestamp\", \"{target}_pred\".\n",
    "        target (str): The target column name (e.g., \"ap_index_nT\" or \"f10.7_index\").\n",
    "        history_days (int): Number of days (or time steps) of historical data to show.\n",
    "            (Here we assume that the OMNI data has daily resolution.)\n",
    "    \"\"\"\n",
    "    # Get the start time of the forecast period from the sat density file.\n",
    "    forecast_start = predicted_sat_df[\"Timestamp\"].min()\n",
    "    \n",
    "    # Define the historical period: the history_days immediately preceding the forecast.\n",
    "    history_start = forecast_start - pd.Timedelta(days=history_days)\n",
    "    \n",
    "    # Filter the historical OMNI data.\n",
    "    history_data = omni_df[(omni_df[\"Timestamp\"] >= history_start) & (omni_df[\"Timestamp\"] < forecast_start)]\n",
    "    \n",
    "    # Create the plot.\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history_data[\"Timestamp\"], history_data[target], label=\"Historical \" + target, color=\"blue\", marker='o')\n",
    "    plt.plot(predicted_sat_df[\"Timestamp\"], predicted_sat_df[target + \"_pred\"], \n",
    "             label=\"Predicted \" + target, color=\"red\", linestyle=\"--\", marker='x')\n",
    "    \n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(target)\n",
    "    plt.title(f\"Historical and Predicted {target} for File ID {predicted_sat_df['File_ID'].iloc[0] if 'File_ID' in predicted_sat_df.columns else 'N/A'}\")\n",
    "    \n",
    "    # Format the x-axis to show full timestamps including hour, minute, second.\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def predict_for_each_timestamp_in_sat_file(\n",
    "    model,\n",
    "    data_handler,\n",
    "    file_id,\n",
    "    omni_df,\n",
    "    input_features,\n",
    "    device,\n",
    "    input_window=100\n",
    "):\n",
    "    \"\"\"\n",
    "    For a given file_id, read the satellite density timestamps.\n",
    "    For each timestamp T in that file:\n",
    "      1) Gather the preceding 'input_window' rows from omni_df (strictly before or up to T)\n",
    "      2) Run a single-step forecast to get ap_index_nT and f10.7_index at time T\n",
    "      3) Store the forecast in new columns (ap_index_nT_pred, f10.7_index_pred)\n",
    "\n",
    "    Args:\n",
    "        model: A trained PatchTST (or similar) model that expects input_window timesteps\n",
    "               and outputs a single-step forecast (1, target_size).\n",
    "        data_handler: Instance of DataHandler with .read_sat_density_data(file_id).\n",
    "        file_id: The specific file ID whose sat density timestamps we want to forecast.\n",
    "        omni_df: The entire OMNI DataFrame used for training (sorted by Timestamp).\n",
    "        input_features: List of column names used as input features for the model.\n",
    "        device: Torch device (e.g., 'cuda' or 'cpu').\n",
    "        input_window: The model’s input window size.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame (the sat density file) with new columns:\n",
    "          \"ap_index_nT_pred\" and \"f10.7_index_pred\"\n",
    "        for each timestamp.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Read the satellite density file for the given file_id.\n",
    "    sat_df = data_handler.read_sat_density_data(file_id).copy()\n",
    "    sat_df[\"Timestamp\"] = pd.to_datetime(sat_df[\"Timestamp\"])\n",
    "    sat_df.sort_values(\"Timestamp\", inplace=True)\n",
    "    sat_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Prepare columns to store predictions.\n",
    "    ap_preds = []\n",
    "    f107_preds = []\n",
    "\n",
    "    # 2) For each timestamp in the satellite file, do a single-step forecast.\n",
    "    for i, row in sat_df.iterrows():\n",
    "        t = row[\"Timestamp\"]\n",
    "        # Gather the preceding input_window rows from omni_df\n",
    "        # that are <= t in time.\n",
    "        # (Ensure omni_df is sorted by Timestamp.)\n",
    "        mask = (omni_df[\"Timestamp\"] <= t)\n",
    "        sub_omni = omni_df[mask].tail(input_window)\n",
    "\n",
    "        # If not enough data to fill input_window, we can't forecast.\n",
    "        # You can decide whether to skip or fill with NaN.\n",
    "        if len(sub_omni) < input_window:\n",
    "            ap_preds.append(np.nan)\n",
    "            f107_preds.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # 3) Build the model input sequence from the sub_omni data\n",
    "        input_seq = sub_omni[input_features].values  # shape: (input_window, len(input_features))\n",
    "\n",
    "        # Run the model for a single-step forecast\n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            # The model should output shape (1, 1, target_size) if it's single-step\n",
    "            # or (1, forecast_horizon, target_size) if multi-step. Here we assume single-step:\n",
    "            out = model(input_tensor)  # shape: (1, 1, target_size) or (1, forecast_horizon, target_size)\n",
    "            out = out.detach().cpu().numpy().squeeze(0)  # shape: (1, target_size) or (forecast_horizon, target_size)\n",
    "\n",
    "        # If your model is truly single-step, out now has shape (1, target_size).\n",
    "        # If multi-step, you might do out = out[0, 0] or something similar.\n",
    "        if len(out.shape) == 2 and out.shape[0] > 1:\n",
    "            # Multi-step model => take only the first step\n",
    "            out = out[0]\n",
    "\n",
    "        ap_preds.append(out[0])     # ap_index_nT\n",
    "        f107_preds.append(out[1])   # f10.7_index\n",
    "\n",
    "    # 4) Store predictions in the sat_df\n",
    "    sat_df[\"ap_index_nT_pred\"] = ap_preds\n",
    "    sat_df[\"f10.7_index_pred\"] = f107_preds\n",
    "\n",
    "    return sat_df\n",
    "# -----------------------------\n",
    "# Example usage:\n",
    "# -----------------------------\n",
    "# Assume you have already generated predicted_sat_df using your predict_for_each_timestamp_in_sat_file function.\n",
    "# For instance:\n",
    "file_id_to_forecast = 10  # change as needed\n",
    "\n",
    "predicted_sat_df = predict_for_each_timestamp_in_sat_file(\n",
    "    model=model,\n",
    "    data_handler=data_handler,\n",
    "    file_id=file_id_to_forecast,\n",
    "    omni_df=omni_df,             # entire OMNI DF used for training\n",
    "    input_features=['Vector_B_Magnitude_nT', 'Dst_index_nT', 'AL_index_nT', 'AU_index_nT', \n",
    "                    'SW_Plasma_Temperature_K', 'BY_nT_GSE', 'Scalar_B_nT', 'BZ_nT_GSE', \n",
    "                    'Lyman_alpha', 'AE_index_nT', 'SW_Plasma_Speed_km_s', 'Bartels_rotation_number', \n",
    "                    'Flow_pressure', 'R_Sunspot_No', 'SW_Proton_Density_N_cm3', 'Kp_index', \n",
    "                    'Plasma_Beta', 'BX_nT_GSE_GSM'],\n",
    "    device=device,\n",
    "    input_window=100\n",
    ")\n",
    "\n",
    "# Optionally, if your predicted_sat_df does not include a \"File_ID\" column, you can add it:\n",
    "predicted_sat_df[\"File_ID\"] = file_id_to_forecast\n",
    "\n",
    "# Now, plot the two separate figures:\n",
    "plot_forecast_for_file(omni_df, predicted_sat_df, target=\"ap_index_nT\", history_days=60)\n",
    "plot_forecast_for_file(omni_df, predicted_sat_df, target=\"f10.7_index\", history_days=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model and Generate Forecasted Sat Density Files\n",
    "\n",
    "In this section, we will:\n",
    "1. Save the trained PatchTST model to a file so it can be imported later.\n",
    "2. Create a new folder (`data/sat_density_omni_forcasted`) for storing the forecasted sat density files.\n",
    "3. For each file ID, we will:\n",
    "   - Run our forecasting function (which uses the entire OMNI dataset to predict the `ap_index_nT` and `f10.7_index` at the sat density timestamps for that file).\n",
    "   - Append the predicted values as new columns to a copy of the original sat density DataFrame.\n",
    "   - Save the new DataFrame to the output folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save the trained model ===\n",
    "model_save_path = Path(\"./saved_models/patchtst_model.pth\")\n",
    "model_save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "logger.info(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# === Create output folder for forecasted sat density files ===\n",
    "output_folder = Path(\"./data/sat_density_omni_forcasted\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Output folder created: {output_folder}\")\n",
    "\n",
    "# === Loop over each file_id and forecast ===\n",
    "file_ids = data_handler.initial_states[\"File ID\"].unique()\n",
    "logger.info(f\"Forecasting for {len(file_ids)} file IDs.\")\n",
    "\n",
    "for fid in file_ids:\n",
    "    try:\n",
    "        # Run the forecasting function for each file_id.\n",
    "        # The function 'predict_for_each_timestamp_in_sat_file' should be defined elsewhere in your notebook.\n",
    "        pred_df = predict_for_each_timestamp_in_sat_file(\n",
    "            model=model,\n",
    "            data_handler=data_handler,\n",
    "            file_id=fid,\n",
    "            omni_df=omni_df,             # Use the full OMNI DataFrame\n",
    "            input_features=input_features,  # The candidate input features (as selected/cleaned earlier)\n",
    "            device=device,\n",
    "            input_window=100\n",
    "        )\n",
    "        # Save the predicted sat density file to the new folder.\n",
    "        output_path = output_folder / f\"sat_density_{fid:05d}.csv\"\n",
    "        pred_df.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Saved forecasted file for File ID {fid} to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error forecasting for File ID {fid}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we:\n",
    "- Loaded and preprocessed the OMNI and sat density data.\n",
    "- Selected candidate features based on domain knowledge and cleaned them.\n",
    "- Defined the PatchTST model and the dataset classes.\n",
    "- Set up training, evaluation, and forecasting functions.\n",
    "- (Optionally) Trained the model and evaluated its performance.\n",
    "- Forecasted on a sample and plotted the historical and forecasted values.\n",
    "\n",
    "Use the individual cells to debug and experiment without running the entire pipeline every time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stormai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
